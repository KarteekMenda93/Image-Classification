# -*- coding: utf-8 -*-
"""Transfer Learning - Image Classification using Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1puUl26ZJdS5xWYnfaGYOszMnM7reC15u

import the packages.

1. tensorflow hub is a repo which contains lot ofpre trained models for image classification and nlp tasks
2. tensorflow dataset is again a repo of datasets.
"""

import matplotlib.pylab as plt
import tensorflow as tf
import tensorflow_hub as hub

import os
import numpy as np

import tensorflow_datasets as tfds

import warnings
warnings.filterwarnings('ignore')

"""# loading the beans dataset.
* classify if the palnt is infected or not.

* with_info=True---print all the metadata we have for this data.
* as_supervised=True---download the target labels for that data.
* split---- 3 splits for that data.

"""

datasets, info = tfds.load(name='beans', with_info=True, as_supervised=True, split=['train','test','validation'])

"""# look at the info about the dataset completely."""

info

"""# lets visualise the dataset."""

test, info_test = tfds.load(name='beans', with_info=True, split='test')
tfds.show_examples(info_test,test)

"""# lets create a function to scale the dataset so that my training converges faster.
here i convert the output pixel value between 0 and 1.

The model i am going to use is the mobilenet model. and mobilenet expects the input image to be of shape (224,224).
Also one hot encoding the target variable
"""

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255.0

  return tf.image.resize(image,[224,224]), tf.one_hot(label, 3)

"""# now let us batch the data.

here i am takingout a batch of size 32, it can be altered depending on your gpu's power.
as our dataset has 3 splits, what i am doing is taking out that split and mapping to the scale function so that the pixesl will be of 224*224. and making sure that all classes will be there so shuffling and then with a batchsize of 32.

* datasets[0]--train.
* dataset[1]--test.
* dataset[2]-- validation
"""

def get_dataset(batch_size=32):
  train_dataset_scaled = datasets[0].map(scale).shuffle(1000).batch(batch_size)
  test_dataset_scaled =  datasets[1].map(scale).batch(batch_size)
  val_dataset_scaled =  datasets[2].map(scale).batch(batch_size)
  return train_dataset_scaled, test_dataset_scaled, val_dataset_scaled

train_dataset, test_dataset, val_dataset = get_dataset()
train_dataset.cache()
val_dataset.cache()

len(list(datasets[0]))

"""# now i am downloading a pre trained model.
as said earlier, i am taking mobilenet version 2. so downloading that weights of that pretrained model.
"""

feature_extractor = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"

"""calling the hub.keras layer and then passing the feature extracted layer and telling the shape is 224,224,3 as the image is already resized."""

feature_extractor_layer = hub.KerasLayer(feature_extractor, input_shape=(224,224,3))

feature_extractor_layer.trainable = False# here i want to train the last layer

"""# now i will take that feature extracted layer and keras sequential layer and add dropout(30 % weights to be zero) to that and give the final output layer for 3 classes as well.(activation=softmax)"""

model = tf.keras.Sequential([
  feature_extractor_layer,
  tf.keras.layers.Dropout(0.3),
  tf.keras.layers.Dense(3,activation='softmax')
])

model.summary()

model.compile(
  optimizer=tf.keras.optimizers.Adam(),
  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
  metrics=['acc'])

"""# now the model is to be validated againsy my validation data just to check whether it is going good on unseen data or not."""

history = model.fit(train_dataset, epochs=100, validation_data=val_dataset)

result=model.evaluate(test_dataset)

"""the accuracy on test dataset is 87.50%"""

for test_sample in datasets[1].take(10):  # just 10 images
  image, label = test_sample[0], test_sample[1]
  image_scaled, label_arr= scale(test_sample[0], test_sample[1])
  image_scaled = np.expand_dims(image_scaled, axis=0)   

  img = tf.keras.preprocessing.image.img_to_array(image)                    
  pred=model.predict(image_scaled)
  print(pred)
  plt.figure()
  plt.imshow(image)
  plt.show()
  print("Actual Label: %s" % info.features["label"].names[label.numpy()])
  print("Predicted Label: %s" % info.features["label"].names[np.argmax(pred)])

"""# lets see the confusion matrix"""

for f0,f1 in datasets[1].map(scale).batch(200):
  y=np.argmax(f1, axis=1)
  y_pred=np.argmax(model.predict(f0),axis=1)
  print(tf.math.confusion_matrix(labels=y, predictions=y_pred, num_classes=3))

"""# in class0-----42 were predicted correct and 1 misclassiification.
# in class-1----31 were predicted correct and 12 were misclassified.
# in class-2----39 were predicted correct and 3 were misclassiifed.

* now we know hwre the majority of misclassification is then we can feed in some more images to the model and get good accuracy.

# lets save our model, so that we can use it in futureif i have any deployment plans for this.
"""

model.save('./models', save_format='tf')